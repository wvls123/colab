{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wvls123/colab/blob/main/%D0%92%D0%B5%D0%B1%D0%B8%D0%BD%D0%B0%D1%80_19_%D0%BD%D0%BE%D1%8F%D0%B1%D1%80%D1%8F_Weweb_%2B_GPT_%D0%94%D0%B5%D0%BD%D1%8C_2_%D0%9A%D0%BE%D0%BB%D0%B0%D0%B1_%D0%B4%D0%B5%D0%BC%D0%BE_%D1%81%D0%B0%D0%B9%D1%82%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FLtd1P_aEqO"
      },
      "source": [
        "НЕЙРО - КОНСУЛЬТАНТ ЧУДО ЧЕРДАЧОК по VC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_fA7N1bnxfZ"
      },
      "source": [
        "### Версия докрученная вручную с голосовым переключением в вевебе"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLupkQocn2oh",
        "outputId": "8afd8ead-3ed1-4503-d61d-330ad0414db9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hЗагружаю базу знаний...\n",
            "[OK] Загружено 38127 символов\n",
            "[CHUNKS] Создано 64 чанков\n",
            "[EMBED] Получено 64 векторов\n",
            "[INDEX] Построен индекс: 64 чанков\n",
            "Освобождаю порт...\n",
            "Запускаю Flask на порту 5001...\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5001\n",
            " * Running on http://172.28.0.12:5001\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Flask запущен!\n",
            "Создаю туннель...\n",
            "\u001b[90m2025-11-19T16:30:17Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-11-19T16:30:17Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m |  https://space-eau-impressive-cardiovascular.trycloudflare.com                             |\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.11.1 (Checksum 83ea55259e419549817460d0c097f23ad1327364d0a63fab2c5463b9283251cb)\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.9, GoArch: amd64\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 no-autoupdate:true protocol:quic url:http://127.0.0.1:5001]\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update if installed by a package manager.\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 01f65238-9fab-43a2-9310-a926ed52ca8e\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.167\n",
            "2025/11/19 16:30:21 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-11-19T16:30:21Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m9b09ecaf-025c-43fa-bed3-52f9e57e0b29 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.167 \u001b[36mlocation=\u001b[0mord10 \u001b[36mprotocol=\u001b[0mquic\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [19/Nov/2025 16:31:57] \"OPTIONS /clear_history HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [19/Nov/2025 16:31:57] \"POST /clear_history HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CLEAR] История 4f122ea2-a5a4-4ed9-9d6c-2ca36ce40439 очищена\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [19/Nov/2025 16:32:04] \"OPTIONS /webhook/text HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[TEXT] 4f122ea2-a5a4-4ed9-9d6c-2ca36ce40439: Интересует организация дня рождения для дочери.... | TTS=False\n",
            "[OK] Загружено 15307 символов\n",
            "[PROMPT] Загружен промпт (15307 символов)\n",
            "[EMBED] Получено 1 векторов\n",
            "[SEARCH] Найдено 7 чанков\n",
            "[PROMPT] История: 1, контекст: 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [19/Nov/2025 16:32:07] \"POST /webhook/text HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Ответ: 167 символов\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [19/Nov/2025 16:32:25] \"OPTIONS /webhook/text HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[TEXT] 4f122ea2-a5a4-4ed9-9d6c-2ca36ce40439: 3 года, 1 августа... | TTS=False\n",
            "[OK] Загружено 15307 символов\n",
            "[PROMPT] Загружен промпт (15307 символов)\n",
            "[EMBED] Получено 1 векторов\n",
            "[SEARCH] Найдено 7 чанков\n",
            "[PROMPT] История: 3, контекст: 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [19/Nov/2025 16:32:28] \"POST /webhook/text HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Ответ: 163 символов\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [19/Nov/2025 16:32:39] \"OPTIONS /webhook/text HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[TEXT] 4f122ea2-a5a4-4ed9-9d6c-2ca36ce40439: 20 человек, из них 10 детей... | TTS=False\n",
            "[OK] Загружено 15307 символов\n",
            "[PROMPT] Загружен промпт (15307 символов)\n",
            "[EMBED] Получено 1 векторов\n",
            "[SEARCH] Найдено 7 чанков\n",
            "[PROMPT] История: 5, контекст: 7\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [19/Nov/2025 16:32:48] \"POST /webhook/text HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] Ответ: 934 символов\n"
          ]
        }
      ],
      "source": [
        "# ====== WEWEB BACKEND (FINAL) ======\n",
        "\n",
        "!pip install -q openai tiktoken faiss-cpu gTTS flask flask-cors requests\n",
        "\n",
        "import os, io, time, json, sqlite3, requests, faiss, numpy as np, tempfile, base64\n",
        "from flask import Flask, request, jsonify, send_file\n",
        "from flask_cors import CORS\n",
        "from pathlib import Path\n",
        "from gtts import gTTS\n",
        "from getpass import getpass\n",
        "from openai import OpenAI\n",
        "from threading import Thread\n",
        "\n",
        "api_key = getpass(\"Введите OpenAI API ключ: \").strip()\n",
        "if not api_key:\n",
        "    raise ValueError(\"Ключ не введён.\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "PORT = 5001\n",
        "LLM_MODEL = \"gpt-4.1\"\n",
        "EMBED_MODEL = \"text-embedding-3-small\"\n",
        "WHISPER_MODEL = \"whisper-1\"\n",
        "CHUNK_SIZE_TOKENS = 5000\n",
        "TOP_K = 7\n",
        "EMBED_DIM = 1536\n",
        "\n",
        "KNOWLEDGE_BASE_URL = \"https://docs.google.com/document/d/1SoaYlYnjz6uAUOmQknRQy-dEcpq8E5GE5b1WqCfpgZM/export?format=txt\"\n",
        "SYSTEM_PROMPT_URL = \"https://docs.google.com/document/d/191VR2bZ2VEFGNtnyA2OcigaT2vYy-1nuRLCqEKf160I/export?format=txt\"\n",
        "\n",
        "INDEX_DIR = Path(\"/content/rag_index\")\n",
        "INDEX_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FAISS_INDEX_PATH = INDEX_DIR / \"faiss.index\"\n",
        "DOCS_META_PATH = INDEX_DIR / \"docs_meta.json\"\n",
        "MEMORY_DB_PATH = INDEX_DIR / \"memory.sqlite\"\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "def init_memory():\n",
        "    conn = sqlite3.connect(MEMORY_DB_PATH)\n",
        "    conn.execute(\"\"\"CREATE TABLE IF NOT EXISTS messages (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        user_id TEXT, role TEXT, text TEXT, timestamp REAL\n",
        "    )\"\"\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def save_message(uid, role, text):\n",
        "    conn = sqlite3.connect(MEMORY_DB_PATH)\n",
        "    conn.execute(\"INSERT INTO messages (user_id, role, text, timestamp) VALUES (?,?,?,?)\",\n",
        "                 (uid, role, text, time.time()))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "\n",
        "def get_messages(uid, limit=30):\n",
        "    conn = sqlite3.connect(MEMORY_DB_PATH)\n",
        "    # Берем последние сообщения и сохраняем их естественный порядок\n",
        "    rows = conn.execute(\"\"\"\n",
        "        SELECT role, text FROM messages\n",
        "        WHERE user_id=?\n",
        "        ORDER BY id ASC  -- ← Меняем на ASC или убираем разворот\n",
        "        LIMIT ?\n",
        "    \"\"\", (uid, limit)).fetchall()\n",
        "    conn.close()\n",
        "    return rows\n",
        "\n",
        "init_memory()\n",
        "\n",
        "def clear_history_all():\n",
        "    conn = sqlite3.connect(MEMORY_DB_PATH)\n",
        "    conn.execute(\"DELETE FROM messages\")\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(\"[CLEAR] Вся история очищена\")\n",
        "\n",
        "def clear_history_user(uid):\n",
        "    conn = sqlite3.connect(MEMORY_DB_PATH)\n",
        "    conn.execute(\"DELETE FROM messages WHERE user_id=?\", (uid,))\n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(f\"[CLEAR] История {uid} очищена\")\n",
        "\n",
        "def trim_history_user(uid, limit=50):\n",
        "    conn = sqlite3.connect(MEMORY_DB_PATH)\n",
        "    rows = conn.execute(\"SELECT id FROM messages WHERE user_id=? ORDER BY id DESC\", (uid,)).fetchall()\n",
        "    if len(rows) > limit:\n",
        "        ids_to_delete = [r[0] for r in rows[limit:]]\n",
        "        conn.executemany(\"DELETE FROM messages WHERE id=?\", [(i,) for i in ids_to_delete])\n",
        "        conn.commit()\n",
        "        print(f\"[TRIM] История {uid} обрезана до {limit}\")\n",
        "    conn.close()\n",
        "\n",
        "def download_text(url):\n",
        "    try:\n",
        "        r = requests.get(url, timeout=10)\n",
        "        r.raise_for_status()\n",
        "        print(f\"[OK] Загружено {len(r.text)} символов\")\n",
        "        return r.text\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Загрузка: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def split_into_chunks(text, size=CHUNK_SIZE_TOKENS):\n",
        "    raw_chunks = [ch.strip() for ch in text.split(\"#\") if ch.strip()]\n",
        "    final_chunks = []\n",
        "    for ch in raw_chunks:\n",
        "        while len(ch) > size:\n",
        "            part = ch[:size]\n",
        "            final_chunks.append(part)\n",
        "            ch = ch[size:]\n",
        "        if ch:\n",
        "            final_chunks.append(ch)\n",
        "    print(f\"[CHUNKS] Создано {len(final_chunks)} чанков\")\n",
        "    return final_chunks\n",
        "\n",
        "def load_docs(url):\n",
        "    txt = download_text(url)\n",
        "    if not txt:\n",
        "        return []\n",
        "    chunks = split_into_chunks(txt)\n",
        "    return [{\"id\": f\"chunk_{i}\", \"text\": ch, \"src\": url} for i, ch in enumerate(chunks)]\n",
        "\n",
        "def load_prompt(url):\n",
        "    t = download_text(url).strip()\n",
        "    print(f\"[PROMPT] Загружен промпт ({len(t)} символов)\")\n",
        "    return t if t else \"Ты вежливый ассистент.\"\n",
        "\n",
        "def embed_texts(texts):\n",
        "    clean_texts = [t for t in texts if isinstance(t, str) and t.strip()]\n",
        "    if not clean_texts:\n",
        "        return np.zeros((0, EMBED_DIM), np.float32)\n",
        "    embeddings = []\n",
        "    for i in range(0, len(clean_texts), 10):\n",
        "        batch = clean_texts[i:i+10]\n",
        "        try:\n",
        "            response = client.embeddings.create(model=EMBED_MODEL, input=batch)\n",
        "            vectors = [np.array(item.embedding, dtype=np.float32) for item in response.data]\n",
        "            embeddings.extend(vectors)\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] Embed: {e}\")\n",
        "            continue\n",
        "    if not embeddings:\n",
        "        return np.zeros((0, EMBED_DIM), np.float32)\n",
        "    arr = np.vstack(embeddings)\n",
        "    print(f\"[EMBED] Получено {arr.shape[0]} векторов\")\n",
        "    return arr\n",
        "\n",
        "def build_index(docs):\n",
        "    embs = embed_texts([d[\"text\"] for d in docs])\n",
        "    faiss.normalize_L2(embs)\n",
        "    index = faiss.IndexFlatIP(EMBED_DIM)\n",
        "    index.add(embs)\n",
        "    faiss.write_index(index, str(FAISS_INDEX_PATH))\n",
        "    json.dump(docs, open(DOCS_META_PATH, \"w\", encoding=\"utf8\"), ensure_ascii=False)\n",
        "    print(f\"[INDEX] Построен индекс: {len(docs)} чанков\")\n",
        "\n",
        "def load_index():\n",
        "    if FAISS_INDEX_PATH.exists() and DOCS_META_PATH.exists():\n",
        "        return faiss.read_index(str(FAISS_INDEX_PATH)), json.load(open(DOCS_META_PATH, encoding=\"utf8\"))\n",
        "    return None, None\n",
        "\n",
        "print(\"Загружаю базу знаний...\")\n",
        "docs = load_docs(KNOWLEDGE_BASE_URL)\n",
        "if docs:\n",
        "    build_index(docs)\n",
        "index, meta = load_index()\n",
        "\n",
        "def retrieve(query, topk=TOP_K):\n",
        "    if not index or not meta:\n",
        "        return []\n",
        "    q = embed_texts([query])\n",
        "    faiss.normalize_L2(q)\n",
        "    _, I = index.search(q, topk)\n",
        "    chosen = [meta[i] for i in I[0] if i < len(meta)]\n",
        "    print(f\"[SEARCH] Найдено {len(chosen)} чанков\")\n",
        "    return chosen\n",
        "\n",
        "def compose_prompt(txt, uid):\n",
        "    sys_prompt = load_prompt(SYSTEM_PROMPT_URL)\n",
        "    rel = retrieve(txt)\n",
        "    hist = get_messages(uid)\n",
        "    parts = [{\"role\": \"system\", \"content\": sys_prompt}]\n",
        "    if hist:\n",
        "        for r, t in hist[-10:]:\n",
        "            parts.append({\"role\": r, \"content\": t})\n",
        "    context_text = \"\\n\\n\".join([d[\"text\"] for d in rel]) if rel else \"\"\n",
        "    user_input = f\"{txt}\\n\\nКонтекст:\\n{context_text}\" if context_text else txt\n",
        "    parts.append({\"role\": \"user\", \"content\": user_input})\n",
        "    print(f\"[PROMPT] История: {len(hist)}, контекст: {len(rel)}\")\n",
        "    return parts\n",
        "\n",
        "def generate_answer(messages):\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=LLM_MODEL,\n",
        "            messages=messages,\n",
        "            temperature=0.5,\n",
        "            max_tokens=700\n",
        "        )\n",
        "        answer = completion.choices[0].message.content.strip()\n",
        "        print(f\"[OK] Ответ: {len(answer)} символов\")\n",
        "        return answer\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] LLM: {e}\")\n",
        "        return \"Произошла ошибка при генерации ответа.\"\n",
        "\n",
        "def transcribe_audio(data):\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as tmp_in:\n",
        "        tmp_in.write(data)\n",
        "        tmp_in.flush()\n",
        "        try:\n",
        "            with open(tmp_in.name, \"rb\") as f:\n",
        "                tr = client.audio.transcriptions.create(model=WHISPER_MODEL, file=f)\n",
        "            text = tr.text.strip()\n",
        "            print(f\"[STT] Распознано: {text[:60]}...\")\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] STT: {e}\")\n",
        "            return \"\"\n",
        "        finally:\n",
        "            os.remove(tmp_in.name)\n",
        "\n",
        "def make_tts(text):\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as tmp_out:\n",
        "        try:\n",
        "            gTTS(text=text, lang=\"ru\").save(tmp_out.name)\n",
        "            print(\"[TTS] Готово\")\n",
        "            return tmp_out.name\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] TTS: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "@app.route(\"/health\")\n",
        "def health():\n",
        "    return jsonify({\"ok\": True, \"time\": time.time()})\n",
        "\n",
        "@app.route(\"/webhook/text\", methods=[\"POST\"])\n",
        "def webhook_text():\n",
        "    try:\n",
        "        data = request.get_json(force=True)\n",
        "        uid = data.get(\"user_id\", \"anon\")\n",
        "        text = data.get(\"text\", \"\").strip()\n",
        "        tts_flag = data.get(\"tts\") in [True, \"true\", \"True\"]\n",
        "\n",
        "        if not text:\n",
        "            return jsonify({\"error\": \"text is empty\"}), 400\n",
        "\n",
        "        print(f\"\\n[TEXT] {uid}: {text[:50]}... | TTS={tts_flag}\")\n",
        "\n",
        "        save_message(uid, \"user\", text)\n",
        "        messages = compose_prompt(text, uid)\n",
        "        answer = generate_answer(messages)\n",
        "        save_message(uid, \"assistant\", answer)\n",
        "\n",
        "        response = {\n",
        "            \"answer\": answer,\n",
        "            \"audio_base64\": None\n",
        "        }\n",
        "\n",
        "        if tts_flag:\n",
        "            path = make_tts(answer)\n",
        "            if path and os.path.exists(path):\n",
        "                with open(path, \"rb\") as f:\n",
        "                    audio_b64 = base64.b64encode(f.read()).decode()\n",
        "                    response[\"audio_base64\"] = f\"data:audio/mpeg;base64,{audio_b64}\"\n",
        "                os.remove(path)\n",
        "                print(f\"[TTS] Добавлено аудио в base64\")\n",
        "\n",
        "        return jsonify(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] /webhook/text: {e}\")\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route(\"/webhook/audio\", methods=[\"POST\"])\n",
        "def webhook_audio():\n",
        "    try:\n",
        "        uid = request.form.get(\"user_id\", \"anon\")\n",
        "        tts_flag = request.form.get(\"tts\") in [True, \"true\", \"True\"]\n",
        "        f = request.files.get(\"file\")\n",
        "\n",
        "        if not f:\n",
        "            return jsonify({\"error\": \"no audio file\"}), 400\n",
        "\n",
        "        print(f\"\\n[AUDIO] {uid} | TTS={tts_flag}\")\n",
        "\n",
        "        text = transcribe_audio(f.read())\n",
        "        save_message(uid, \"user\", text)\n",
        "        messages = compose_prompt(text, uid)\n",
        "        answer = generate_answer(messages)\n",
        "        save_message(uid, \"assistant\", answer)\n",
        "\n",
        "        response = {\n",
        "            \"transcription\": text,\n",
        "            \"answer\": answer,\n",
        "            \"audio_base64\": None\n",
        "        }\n",
        "\n",
        "        if tts_flag:\n",
        "            path = make_tts(answer)\n",
        "            if path and os.path.exists(path):\n",
        "                with open(path, \"rb\") as f:\n",
        "                    audio_b64 = base64.b64encode(f.read()).decode()\n",
        "                    response[\"audio_base64\"] = f\"data:audio/mpeg;base64,{audio_b64}\"\n",
        "                os.remove(path)\n",
        "                print(f\"[TTS] Добавлено аудио в base64\")\n",
        "\n",
        "        return jsonify(response)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] /webhook/audio: {e}\")\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route(\"/reload_kb\", methods=[\"POST\"])\n",
        "def reload_kb():\n",
        "    global KNOWLEDGE_BASE_URL, SYSTEM_PROMPT_URL, index, meta\n",
        "    try:\n",
        "        data = request.get_json(force=True)\n",
        "        KNOWLEDGE_BASE_URL = data.get(\"kb_url\", KNOWLEDGE_BASE_URL)\n",
        "        SYSTEM_PROMPT_URL = data.get(\"prompt_url\", SYSTEM_PROMPT_URL)\n",
        "        docs = load_docs(KNOWLEDGE_BASE_URL)\n",
        "        if docs:\n",
        "            build_index(docs)\n",
        "            index, meta = load_index()\n",
        "        return jsonify({\"status\": \"reloaded\", \"chunks\": len(meta) if meta else 0})\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] /reload_kb: {e}\")\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route(\"/clear_history\", methods=[\"POST\"])\n",
        "def clear_history():\n",
        "    try:\n",
        "        data = request.get_json(force=True)\n",
        "        uid = data.get(\"user_id\")\n",
        "        mode = data.get(\"mode\", \"user\")\n",
        "        limit = data.get(\"limit\")\n",
        "        if mode == \"all\":\n",
        "            clear_history_all()\n",
        "            return jsonify({\"status\": \"ok\", \"cleared\": \"all\"})\n",
        "        elif limit:\n",
        "            trim_history_user(uid, int(limit))\n",
        "            return jsonify({\"status\": \"ok\", \"trimmed_to\": limit})\n",
        "        else:\n",
        "            clear_history_user(uid)\n",
        "            return jsonify({\"status\": \"ok\", \"cleared\": uid})\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] /clear_history: {e}\")\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "print(\"Освобождаю порт...\")\n",
        "!fuser -k {PORT}/tcp 2>/dev/null || true\n",
        "\n",
        "print(f\"Запускаю Flask на порту {PORT}...\")\n",
        "Thread(target=lambda: app.run(host=\"0.0.0.0\", port=PORT, debug=False), daemon=True).start()\n",
        "\n",
        "time.sleep(5)\n",
        "print(\"Flask запущен!\")\n",
        "\n",
        "print(\"Создаю туннель...\")\n",
        "!wget -q -O cloudflared.deb https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb\n",
        "!dpkg -i cloudflared.deb > /dev/null 2>&1\n",
        "!cloudflared tunnel --url http://127.0.0.1:{PORT} --no-autoupdate"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}